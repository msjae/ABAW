{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from positional_encoding import PositionalEncoding  # Make sure to import from the correct file or directory\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, seq_len, embedding_size, nhead, num_encoder_layers, num_classes, cfg):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding_size = embedding_size\n",
    "        self.nhead = nhead\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.positional_encoding = PositionalEncoding(self.embedding_size)\n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model=self.embedding_size, nhead=self.nhead, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=self.num_encoder_layers)\n",
    "        self.Linear = nn.Sequential(\n",
    "            nn.Linear(self.seq_len * self.embedding_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, self.num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, embedding_size)\n",
    "        self.batch_size = x.shape[0]\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.view(self.batch_size, -1)\n",
    "        x = self.Linear(x)\n",
    "        if self.cfg['task'] == \"VA\":\n",
    "            x = torch.tanh(x)\n",
    "        elif self.cfg['task'] == \"EXPR\":\n",
    "            x = torch.softmax(x, dim=1)\n",
    "        else:\n",
    "            x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(50, 768, 8, 6, 8, {'task': 'EXPR'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 50, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
